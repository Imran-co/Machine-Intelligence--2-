{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeOPYexcdQNutb5y9CeaNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imran-co/Machine-Intelligence--2-/blob/main/Lab_cycle2_q4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Implement a Python program to solve a simple 5x5 grid world environment using\n",
        "dynamic programming techniques. The environment consists of an agent that\n",
        "starts at the top-left corner and aims to reach the bottom-right cell, with possible\n",
        "action set {up, down, left, right}. Define the transition model and reward\n",
        "structure (reward of +1 for reaching the goal, 0 otherwise). Apply value iteration\n",
        "and policy iteration separately to compute the optimal value function for the\n",
        "agent to reach the goal."
      ],
      "metadata": {
        "id": "BmIQgKSs6YBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "dimension = 5\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "#value function\n",
        "gamma = 0.9\n",
        "rewardValue_g = 1\n",
        "rewardValue_n = 0\n",
        "numIterations = 1000\n",
        "\n",
        "initial_state = (0, 0)\n",
        "goal_state = (dimension - 1, dimension - 1)\n",
        "\n",
        "\n",
        "\n",
        "def get_reward(state):\n",
        "  if state == goal_state:\n",
        "    return rewardValue_g\n",
        "  else:\n",
        "    return rewardValue_n\n",
        "\n",
        "def transition(state, action):\n",
        "  row, col = state\n",
        "  if action == 'up':\n",
        "    next_row, next_col = row - 1, col\n",
        "  elif action == 'down':\n",
        "    next_row, next_col = row + 1, col\n",
        "  elif action == 'left':\n",
        "    next_row, next_col = row, col - 1\n",
        "  elif action == 'right':\n",
        "    next_row, next_col = row, col + 1\n",
        "  else:\n",
        "    next_row, next_col = row, col # Stay in the same state for invalid actions\n",
        "\n",
        "  # Handle boundary conditions\n",
        "  next_row = max(0, min(next_row, dimension -1))\n",
        "  next_col = max(0, min(next_col, dimension -1))\n",
        "\n",
        "  return (next_row, next_col)\n",
        "\n",
        "def value_iteration():\n",
        "  value_function = np.zeros((dimension, dimension))\n",
        "  policy = np.zeros((dimension, dimension), dtype=str)\n",
        "\n",
        "  for _ in range(numIterations):\n",
        "      updated_value_function = np.copy(value_function)\n",
        "      for row in range(dimension):\n",
        "          for col in range(dimension):\n",
        "              state = (row, col)\n",
        "              if state == goal_state:\n",
        "                  continue\n",
        "              value_actions = []\n",
        "              for action in actions:\n",
        "                  next_state = transition(state, action)\n",
        "                  reward = get_reward(next_state)\n",
        "                  value_action = reward + gamma * value_function[next_state]\n",
        "                  value_actions.append(value_action)\n",
        "              updated_value_function[state] = max(value_actions)\n",
        "      value_function = updated_value_function\n",
        "\n",
        "  # Derive policy from value function\n",
        "  for row in range(dimension):\n",
        "      for col in range(dimension):\n",
        "          state = (row, col)\n",
        "          if state == goal_state:\n",
        "              continue\n",
        "          value_actions = []\n",
        "          for action in actions:\n",
        "              next_state = transition(state, action)\n",
        "              reward = get_reward(next_state)\n",
        "              value_action = reward + gamma * value_function[next_state]\n",
        "              value_actions.append(value_action)\n",
        "          best_action_index = value_actions.index(max(value_actions))\n",
        "          policy[state] = actions[best_action_index]\n",
        "\n",
        "  return value_function, policy\n",
        "\n",
        "\n",
        "def policy_iteration():\n",
        "  value_function = np.zeros((dimension, dimension))\n",
        "  policy = np.zeros((dimension, dimension), dtype=str)\n",
        "\n",
        "  for _ in range(numIterations):\n",
        "    updated_value_function = np.copy(value_function)\n",
        "    for row in range(dimension):\n",
        "      for col in range(dimension):\n",
        "        state = (row, col)\n",
        "        if state == goal_state:\n",
        "          continue\n",
        "        value_actions = []\n",
        "        for action in actions:\n",
        "          next_state = transition(state, action)\n",
        "          reward = get_reward(next_state)\n",
        "          value_action = reward + gamma * value_function[next_state]\n",
        "          value_actions.append(value_action)\n",
        "\n",
        "        best_value = max(value_actions)\n",
        "        updated_value_function[state] = best_value\n",
        "\n",
        "        best_action_index = value_actions.index(best_value)\n",
        "        policy[state] = actions[best_action_index]\n",
        "\n",
        "\n",
        "    value_function = updated_value_function # Update the value function for the next iteration\n",
        "\n",
        "  return value_function, policy"
      ],
      "metadata": {
        "id": "b58beZcMzjcd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what the letters mean:\n",
        "\n",
        "'d': Move down\n",
        "'r': Move right\n",
        "'': This is the goal state, so no action is needed."
      ],
      "metadata": {
        "id": "fo8suOfb6PJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Value Iteration:\")\n",
        "value_function, policy = value_iteration()\n",
        "print(value_function)\n",
        "print(\"Policy Iteration:\")\n",
        "value_function, policy = policy_iteration()\n",
        "print(value_function)\n",
        "print(\"Optimal Policy : \")\n",
        "print(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly0GRVEG8nYs",
        "outputId": "114b213d-a738-46b6-9c28-38cc6e5380cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration:\n",
            "[[0.4782969 0.531441  0.59049   0.6561    0.729    ]\n",
            " [0.531441  0.59049   0.6561    0.729     0.81     ]\n",
            " [0.59049   0.6561    0.729     0.81      0.9      ]\n",
            " [0.6561    0.729     0.81      0.9       1.       ]\n",
            " [0.729     0.81      0.9       1.        0.       ]]\n",
            "Policy Iteration:\n",
            "[[0.4782969 0.531441  0.59049   0.6561    0.729    ]\n",
            " [0.531441  0.59049   0.6561    0.729     0.81     ]\n",
            " [0.59049   0.6561    0.729     0.81      0.9      ]\n",
            " [0.6561    0.729     0.81      0.9       1.       ]\n",
            " [0.729     0.81      0.9       1.        0.       ]]\n",
            "Optimal Policy : \n",
            "[['d' 'd' 'd' 'd' 'd']\n",
            " ['d' 'd' 'd' 'd' 'd']\n",
            " ['d' 'd' 'd' 'd' 'd']\n",
            " ['d' 'd' 'd' 'd' 'd']\n",
            " ['r' 'r' 'r' 'r' '']]\n"
          ]
        }
      ]
    }
  ]
}