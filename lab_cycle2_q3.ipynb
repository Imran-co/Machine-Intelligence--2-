{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTYdBFfndbzw74Tg5IjvxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imran-co/Machine-Intelligence--2-/blob/main/lab_cycle2_q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to model a Markov Decision Process (MDP) with 3\n",
        "states and 2 actions, using a predefined transition probability matrix and reward\n",
        "function. Initialize a uniform stochastic policy where each action has a\n",
        "probability of 0.5 in every state. Compute the value function for each state under\n",
        "this policy (policy evaluation) using the Bellman expectation equation, with a\n",
        "discount factor of 0.9. Provide a sample MDP with explicit transition and reward\n",
        "values, and demonstrate the calculated value function."
      ],
      "metadata": {
        "id": "b7SBRKI6dQeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJMTUOaEW-9X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "  def __init__(self, states, actions, rewards, transition, gamma =0.9):\n",
        "    self.states = states\n",
        "    self.actions = actions\n",
        "    self.rewards = rewards\n",
        "    self.transition = transition\n",
        "    self.gamma = gamma\n",
        "  def policy_evaluation(self, policy, theta=1e-6, max_iterations=1000):\n",
        "    Vf = {s: 0 for s in self.states}\n",
        "    for i in range(max_iterations):\n",
        "      delta = 0\n",
        "      for s in self.states:\n",
        "        v = Vf[s]\n",
        "        new_v = 0\n",
        "        for a in self.actions:\n",
        "          action_prob = policy[s][a]\n",
        "          for s_ in self.states:\n",
        "            new_v += action_prob * self.transition[(s, a, s_)] * (self.rewards[(s, a, s_)] + self.gamma * Vf[s_])\n",
        "\n",
        "          Vf[s] = new_v\n",
        "          delta = max(delta, abs(v - Vf[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "          break\n",
        "\n",
        "      return Vf\n",
        "\n",
        "states = ['s1', 's2', 's3']\n",
        "actions = ['a1', 'a2']\n",
        "\n",
        "\n",
        "transitions = {\n",
        "    # From s1\n",
        "    ('s1', 'a1', 's1'): 0.2, ('s1', 'a1', 's2'): 0.7, ('s1', 'a1', 's3'): 0.1,\n",
        "    ('s1', 'a2', 's1'): 0.6, ('s1', 'a2', 's2'): 0.3, ('s1', 'a2', 's3'): 0.1,\n",
        "\n",
        "    # From s2\n",
        "    ('s2', 'a1', 's1'): 0.1, ('s2', 'a1', 's2'): 0.8, ('s2', 'a1', 's3'): 0.1,\n",
        "    ('s2', 'a2', 's1'): 0.3, ('s2', 'a2', 's2'): 0.4, ('s2', 'a2', 's3'): 0.3,\n",
        "\n",
        "    # From s3\n",
        "    ('s3', 'a1', 's1'): 0.4, ('s3', 'a1', 's2'): 0.4, ('s3', 'a1', 's3'): 0.2,\n",
        "    ('s3', 'a2', 's1'): 0.1, ('s3', 'a2', 's2'): 0.1, ('s3', 'a2', 's3'): 0.8,\n",
        "}\n",
        "\n",
        "\n",
        "rewards = {\n",
        "    # From s1\n",
        "    ('s1', 'a1', 's1'): 1, ('s1', 'a1', 's2'): 2, ('s1', 'a1', 's3'): -1,\n",
        "    ('s1', 'a2', 's1'): 0, ('s1', 'a2', 's2'): 1, ('s1', 'a2', 's3'): 1,\n",
        "\n",
        "    # From s2\n",
        "    ('s2', 'a1', 's1'): 1, ('s2', 'a1', 's2'): 3, ('s2', 'a1', 's3'): 0,\n",
        "    ('s2', 'a2', 's1'): -1, ('s2', 'a2', 's2'): 2, ('s2', 'a2', 's3'): 1,\n",
        "\n",
        "    # From s3\n",
        "    ('s3', 'a1', 's1'): 3, ('s3', 'a1', 's2'): 1, ('s3', 'a1', 's3'): 0,\n",
        "    ('s3', 'a2', 's1'): -2, ('s3', 'a2', 's2'): 0, ('s3', 'a2', 's3'): 2,\n",
        "}\n",
        "\n",
        "mdp = MDP(states, actions, transitions, rewards, gamma=0.9)\n",
        "\n",
        "uniform_policy = {\n",
        "    's1': {'a1': 0.5, 'a2': 0.5},\n",
        "    's2': {'a1': 0.5, 'a2': 0.5},\n",
        "    's3': {'a1': 0.5, 'a2': 0.5}\n",
        "}\n",
        "\n",
        "vf = mdp.policy_evaluation(uniform_policy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MDP with 3 states and 2 actions:\")\n",
        "print(\"States:\", states)\n",
        "print(\"Actions:\", actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvjnK0gga1Wq",
        "outputId": "bf209913-b3f5-4fac-a2ab-c01109956de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDP with 3 states and 2 actions:\n",
            "States: ['s1', 's2', 's3']\n",
            "Actions: ['a1', 'a2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"P(s2|s1,a1) =\", transitions[('s1', 'a1', 's2')])\n",
        "print(\"P(s3|s2,a2) =\", transitions[('s2', 'a2', 's3')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bf-UD6ba-OA",
        "outputId": "9f46cb84-c73b-4c5e-a52a-c90c5b6da484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(s2|s1,a1) = 0.7\n",
            "P(s3|s2,a2) = 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R(s1,a1,s2) =\", rewards[('s1', 'a1', 's2')])\n",
        "print(\"R(s3,a2,s1) =\", rewards[('s3', 'a2', 's1')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPrHJHEKbioj",
        "outputId": "2f9a7e8b-3b94-40ad-c071-c5e1a62a24cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R(s1,a1,s2) = 2\n",
            "R(s3,a2,s1) = -2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for state, action_probs in uniform_policy.items():\n",
        "    print(f\"{state}: {action_probs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7HHxwUbbo_8",
        "outputId": "e2dbf48b-ba25-4a92-8f88-9a0bd132011f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s1: {'a1': 0.5, 'a2': 0.5}\n",
            "s2: {'a1': 0.5, 'a2': 0.5}\n",
            "s3: {'a1': 0.5, 'a2': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for state, value in vf.items():\n",
        "    print(f\"V({state}) = {value:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaAUrWSCc-y_",
        "outputId": "3f96c74d-fdfd-450c-de1c-0904df6a4175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V(s1) = 0.950\n",
            "V(s2) = 3.160\n",
            "V(s3) = 6.503\n"
          ]
        }
      ]
    }
  ]
}